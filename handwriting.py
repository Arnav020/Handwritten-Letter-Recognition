# -*- coding: utf-8 -*-
"""Handwriting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CJlCsMTnqrjlyeKKPDmPqKs4DNEWPtqf
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import os

import numpy as np
from six.moves import urllib
from tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf
import tensorflow_datasets.public_api as tfds





""" THERE WAS A PROBLEM IN DIRECTLY LOADING EMNIST DATASET, SO THE CODE FROM DOCUMENTATION OF TENSORFLOW WAS DIRECTLY USED to LOAD THE EMNIST DATASET"""





# MNIST constants
# CVDF mirror of http://yann.lecun.com/exdb/mnist/
_MNIST_URL = "https://storage.googleapis.com/cvdf-datasets/mnist/"
_MNIST_TRAIN_DATA_FILENAME = "train-images-idx3-ubyte.gz"
_MNIST_TRAIN_LABELS_FILENAME = "train-labels-idx1-ubyte.gz"
_MNIST_TEST_DATA_FILENAME = "t10k-images-idx3-ubyte.gz"
_MNIST_TEST_LABELS_FILENAME = "t10k-labels-idx1-ubyte.gz"
_MNIST_IMAGE_SIZE = 28
MNIST_IMAGE_SHAPE = (_MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, 1)
MNIST_NUM_CLASSES = 10
_TRAIN_EXAMPLES = 60000
_TEST_EXAMPLES = 10000

class MNIST(tfds.core.GeneratorBasedBuilder):
  """MNIST."""

  URL = _MNIST_URL

  VERSION = tfds.core.Version("3.0.1")

  def _info(self):
    return tfds.core.DatasetInfo(
        builder=self,
        description="The MNIST database of handwritten digits.",
        features=tfds.features.FeaturesDict({
            "image": tfds.features.Image(shape=MNIST_IMAGE_SHAPE),
            "label": tfds.features.ClassLabel(num_classes=MNIST_NUM_CLASSES),
        }),
        supervised_keys=("image", "label"),
        homepage="http://yann.lecun.com/exdb/mnist/",
        citation=_MNIST_CITATION,
    )


class EMNISTConfig(tfds.core.BuilderConfig):
  """BuilderConfig for EMNIST CONFIG."""

  def __init__(self, *, class_number, train_examples, test_examples, **kwargs):
    """BuilderConfig for EMNIST class number.

    Args:
      class_number: There are six different splits provided in this dataset. And
        have different class numbers.
      train_examples: number of train examples
      test_examples: number of test examples
      **kwargs: keyword arguments forwarded to super.
    """
    super(EMNISTConfig, self).__init__(**kwargs)
    self.class_number = class_number
    self.train_examples = train_examples
    self.test_examples = test_examples

class EMNIST(MNIST):
  """Emnist dataset."""

  URL = "https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip"
  VERSION = tfds.core.Version("3.1.0")
  RELEASE_NOTES = {
      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
      "3.1.0": "Updated broken download URL",
  }
  BUILDER_CONFIGS = [
      EMNISTConfig(
          name="byclass",
          class_number=62,
          train_examples=697932,
          test_examples=116323,
          description="EMNIST ByClass",
      ),
      EMNISTConfig(
          name="bymerge",
          class_number=47,
          train_examples=697932,
          test_examples=116323,
          description="EMNIST ByMerge",
      ),
      EMNISTConfig(
          name="balanced",
          class_number=47,
          train_examples=112800,
          test_examples=18800,
          description="EMNIST Balanced",
      ),
      EMNISTConfig(
          name="letters",
          class_number=37,
          train_examples=88800,
          test_examples=14800,
          description="EMNIST Letters",
      ),
      EMNISTConfig(
          name="digits",
          class_number=10,
          train_examples=240000,
          test_examples=40000,
          description="EMNIST Digits",
      ),
      EMNISTConfig(
          name="mnist",
          class_number=10,
          train_examples=60000,
          test_examples=10000,
          description="EMNIST MNIST",
      ),
  ]

  def _info(self):
    return tfds.core.DatasetInfo(
        builder=self,
        description=(
            "The EMNIST dataset is a set of handwritten character digits "
            "derived from the NIST Special Database 19 and converted to "
            "a 28x28 pixel image format and dataset structure that directly "
            "matches the MNIST dataset.\n\n"
            "Note: Like the original EMNIST data, images provided here are "
            "inverted horizontally and rotated 90 anti-clockwise. You can use "
            "`tf.transpose` within `ds.map` to convert the images to a "
            "human-friendlier format."
        ),
        features=tfds.features.FeaturesDict({
            "image": tfds.features.Image(shape=MNIST_IMAGE_SHAPE),
            "label": tfds.features.ClassLabel(
                num_classes=self.builder_config.class_number
            ),
        }),
        supervised_keys=("image", "label"),
        homepage=(
            "https://www.nist.gov/itl/products-and-services/emnist-dataset"
        ),
        citation=_EMNIST_CITATION,
    )

  def _split_generators(self, dl_manager):
    filenames = {
        "train_data": "emnist-{}-train-images-idx3-ubyte.gz".format(
            self.builder_config.name
        ),
        "train_labels": "emnist-{}-train-labels-idx1-ubyte.gz".format(
            self.builder_config.name
        ),
        "test_data": "emnist-{}-test-images-idx3-ubyte.gz".format(
            self.builder_config.name
        ),
        "test_labels": "emnist-{}-test-labels-idx1-ubyte.gz".format(
            self.builder_config.name
        ),
    }

    dir_name = os.path.join(dl_manager.download_and_extract(self.URL), "gzip")
    extracted = dl_manager.extract(
        {k: os.path.join(dir_name, fname) for k, fname in filenames.items()}
    )

    return [
        tfds.core.SplitGenerator(
            name=tfds.Split.TRAIN,
            gen_kwargs=dict(
                num_examples=self.builder_config.train_examples,
                data_path=extracted["train_data"],
                label_path=extracted["train_labels"],
            ),
        ),
        tfds.core.SplitGenerator(
            name=tfds.Split.TEST,
            gen_kwargs=dict(
                num_examples=self.builder_config.test_examples,
                data_path=extracted["test_data"],
                label_path=extracted["test_labels"],
            ),
        ),
    ]

# 70000 images in MNIST dataset

import tensorflow as tf
import tensorflow_datasets as tfds

def load_emnist_minimal(split='digits', batch_size=32):
    """
    Minimal EMNIST dataset loader for debugging
    """
    try:
        # Step 1: Create builder and print info
        builder = tfds.builder(f'emnist/{split}')
        print("Builder created successfully")
        print(f"Builder info: {builder.info}")

        # Step 2: Download data
        print("\nDownloading and preparing dataset...")
        builder.download_and_prepare()
        print("Download and prepare completed")

        # Step 3: Get data source
        print("\nGetting data source...")
        train_ds = builder.as_data_source(split='train')
        print("Got train data source")

        # Step 4: Try to read one example
        print("\nTrying to read one example...")
        first_example = next(iter(train_ds))
        print(f"First example keys: {first_example.keys()}")

        # Step 5: Convert single example
        print("\nConverting single example...")
        image = tf.cast(first_example['image'], tf.float32) / 255.0
        label = first_example['label']
        print(f"Successfully converted example - Image shape: {image.shape}, Label: {label}")

        return True

    except Exception as e:
        print(f"\nError occurred at step: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("Testing minimal EMNIST loader...")
    success = load_emnist_minimal('digits')
    if success:
        print("\nBasic loading test successful!")
    else:
        print("\nLoading test failed!")

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np

def load_emnist_complete(split='digits', batch_size=32):
    """
    Complete EMNIST dataset loader with proper preprocessing
    """
    try:
        # Create builder
        builder = tfds.builder(f'emnist/{split}')

        # Get data sources
        train_ds = builder.as_data_source(split='train')
        test_ds = builder.as_data_source(split='test')

        def process_dataset(data_source, is_training=True):
            # Convert to numpy arrays first
            images = []
            labels = []

            print(f"Processing {'training' if is_training else 'test'} data...")
            for example in data_source:
                # Normalize image to [0, 1] range
                image = tf.cast(example['image'], tf.float32) / 255.0
                # Rotate image 90 degrees counterclockwise since EMNIST is rotated
                image = tf.image.rot90(image)
                images.append(image.numpy())
                # Label is already an integer, no need for numpy()
                labels.append(example['label'])

            # Stack into arrays
            images = np.stack(images)
            labels = np.array(labels)

            print(f"Processed {len(images)} examples")

            # Create TensorFlow dataset
            dataset = tf.data.Dataset.from_tensor_slices((images, labels))

            if is_training:
                dataset = dataset.shuffle(10000)

            # Batch and prefetch
            dataset = dataset.batch(batch_size)
            dataset = dataset.prefetch(tf.data.AUTOTUNE)

            return dataset

        # Process both datasets
        train_dataset = process_dataset(train_ds, is_training=True)
        test_dataset = process_dataset(test_ds, is_training=False)

        # Get metadata
        num_classes = builder.info.features['label'].num_classes
        input_shape = builder.info.features['image'].shape
        num_train = builder.info.splits['train'].num_examples
        num_test = builder.info.splits['test'].num_examples

        metadata = {
            'num_classes': num_classes,
            'input_shape': input_shape,
            'num_train': num_train,
            'num_test': num_test
        }

        return train_dataset, test_dataset, metadata

    except Exception as e:
        print(f"Error loading EMNIST dataset: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def visualize_samples(dataset, num_samples=9):
    """
    Visualize samples from the dataset
    """
    import matplotlib.pyplot as plt

    # Get one batch
    for images, labels in dataset.take(1):
        # Create a figure
        plt.figure(figsize=(10, 10))
        for i in range(min(num_samples, len(images))):
            plt.subplot(3, 3, i + 1)
            plt.imshow(images[i].numpy().squeeze(), cmap='gray')
            plt.title(f'Label: {labels[i].numpy()}')
            plt.axis('off')
        plt.tight_layout()
        plt.show()

if __name__ == "__main__":
    print("Loading EMNIST balanced dataset...")
    train_ds, test_ds, metadata = load_emnist_complete('digits')

    if train_ds is not None:
        print("\nDataset metadata:")
        for key, value in metadata.items():
            print(f"{key}: {value}")

        print("\nVerifying data shapes...")
        for images, labels in train_ds.take(1):
            print(f"Batch image shape: {images.shape}")
            print(f"Batch label shape: {labels.shape}")
            print(f"Image value range: [{tf.reduce_min(images):.2f}, {tf.reduce_max(images):.2f}]")
            print(f"Sample labels: {labels[:5].numpy()}")

        print("\nVisualizing sample images...")
        visualize_samples(train_ds)

def get_emnist_splits(split='digits', batch_size=32):
    # Load the dataset
    train_ds, test_ds, metadata = load_emnist_complete(split, batch_size)

    if train_ds is None:
        return None, None, None, None

    # Convert TF datasets to numpy arrays
    x_train, y_train = [], []
    x_test, y_test = [], []

    # Extract training data
    for images, labels in train_ds:
        x_train.append(images.numpy())
        y_train.append(labels.numpy())

    # Extract test data
    for images, labels in test_ds:
        x_test.append(images.numpy())
        y_test.append(labels.numpy())

    # Concatenate batches
    x_train = np.concatenate(x_train)
    y_train = np.concatenate(y_train)
    x_test = np.concatenate(x_test)
    y_test = np.concatenate(y_test)

    # Reshape images to 2D if needed
    x_train = x_train.reshape(x_train.shape[0], 28, 28)
    x_test = x_test.reshape(x_test.shape[0], 28, 28)

    return x_train, x_test, y_train, y_test

# Usage example:
x_train, x_test, y_train, y_test = get_emnist_splits('digits')
print(f"Training shapes: {x_train.shape}, {y_train.shape}")
print(f"Test shapes: {x_test.shape}, {y_test.shape}")

some_digit=x_train[810]
some_digit_image=some_digit.reshape(28,28)
plt.imshow(some_digit_image,cmap="binary")
plt.axis("off")
plt.show()

y_train[810]

print("Size of the dataset:", len(x_train) + len(x_test))
print("Number of elements in the training set:", len(x_train))
print("Number of elements in the test set:", len(x_test))

# Get the split data
#x_train, x_test, y_train, y_test = get_emnist_splits('balanced')

# Reshape to 1D arrays (flatten the images)
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Normalize the data
x_train = x_train / 255.0
x_test = x_test / 255.0

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train, y_train)

y_pred = knn.predict(x_test)

accuracy = knn.score(x_test, y_test)
print(f"Test accuracy with 3 neighbors: {accuracy:.4f}")

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(15, 15))  # Larger figure since EMNIST has more classes
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - EMNIST Balanced")
plt.show()

# Get the split data
#x_train, x_test, y_train, y_test = get_emnist_splits('balanced')

# Reshape to 1D arrays (flatten the images)
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Normalize the data
x_train = x_train / 255.0
x_test = x_test / 255.0

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)

y_pred = knn.predict(x_test)

accuracy = knn.score(x_test, y_test)
print(f"Test accuracy with 5 neighbors: {accuracy:.4f}")

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(15, 15))  # Larger figure since EMNIST has more classes
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - EMNIST Balanced")
plt.show()

# Import required libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import numpy as np

def train_svm(x_train, x_test, y_train, y_test):
    """Train and evaluate SVM model"""
    print("\nTraining SVM model...")

    # Take a subset of data for SVM due to computational constraints
    x_train_sub, _, y_train_sub, _ = train_test_split(
        x_train, y_train, train_size=10000, random_state=42, stratify=y_train
    )
    x_test_sub, _, y_test_sub, _ = train_test_split(
        x_test, y_test, train_size=2000, random_state=42, stratify=y_test
    )

    # Create and train SVM model
    svm = SVC(kernel='linear', C=1.0, random_state=42)
    svm.fit(x_train_sub, y_train_sub)

    # Make predictions
    y_pred = svm.predict(x_test_sub)

    # Calculate accuracy
    accuracy = accuracy_score(y_test_sub, y_pred)
    print(f"SVM Test accuracy: {accuracy:.4f}")

    # Create confusion matrix
    cm = confusion_matrix(y_test_sub, y_pred)
    plt.figure(figsize=(15, 15))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix - SVM (EMNIST)")
    plt.show()

def train_logistic_regression(x_train, x_test, y_train, y_test):
    """Train and evaluate Logistic Regression model"""
    print("\nTraining Logistic Regression model...")

    # Create and train Logistic Regression model
    logreg = LogisticRegression(max_iter=1000)
    logreg.fit(x_train, y_train)

    # Make predictions
    y_pred = logreg.predict(x_test)

    # Calculate accuracy
    accuracy = logreg.score(x_test, y_test)
    print(f"Logistic Regression Test accuracy: {accuracy:.4f}")

    # Create confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(15, 15))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix - Logistic Regression (EMNIST)")
    plt.show()

def train_decision_tree(x_train, x_test, y_train, y_test):
    """Train and evaluate Decision Tree model"""
    print("\nTraining Decision Tree model...")

    # Create and train Decision Tree model
    dt_classifier = DecisionTreeClassifier(random_state=42)
    dt_classifier.fit(x_train, y_train)

    # Make predictions
    y_pred = dt_classifier.predict(x_test)

    # Calculate accuracy
    accuracy = dt_classifier.score(x_test, y_test)
    print(f"Decision Tree Test accuracy: {accuracy:.4f}")

    # Create confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(15, 15))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix - Decision Tree (EMNIST)")
    plt.show()

def train_cnn(x_train, x_test, y_train, y_test):
    """Train and evaluate CNN model"""
    print("\nTraining CNN model...")

    # Reshape data for CNN
    x_train_cnn = x_train.reshape(-1, 28, 28, 1)
    x_test_cnn = x_test.reshape(-1, 28, 28, 1)

    # Create CNN model
    model = keras.Sequential([
        keras.Input(shape=(28, 28, 1)),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(47, activation="softmax")  # 47 classes for EMNIST balanced
    ])

    # Compile model
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer="adam",
        metrics=["accuracy"]
    )

    # Train model
    model.fit(
        x_train_cnn, y_train,
        batch_size=128,
        epochs=17,
        validation_split=0.1
    )

    # Evaluate model
    loss, accuracy = model.evaluate(x_test_cnn, y_test, verbose=0)
    print(f"CNN Test loss: {loss:.4f}")
    print(f"CNN Test accuracy: {accuracy:.4f}")

    # Create confusion matrix
    y_pred = np.argmax(model.predict(x_test_cnn), axis=1)
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(15, 15))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix - CNN (EMNIST)")
    plt.show()

# Main execution
if __name__ == "__main__":
    # Get the split data using your existing function
    x_train, x_test, y_train, y_test = get_emnist_splits('digits')

    # Reshape to 1D arrays for traditional ML models
    x_train_flat = x_train.reshape(x_train.shape[0], -1)
    x_test_flat = x_test.reshape(x_test.shape[0], -1)

    # Normalize the data
    x_train_flat = x_train_flat / 255.0
    x_test_flat = x_test_flat / 255.0

    # Train and evaluate each model
    train_svm(x_train_flat, x_test_flat, y_train, y_test)
    train_logistic_regression(x_train_flat, x_test_flat, y_train, y_test)
    train_decision_tree(x_train_flat, x_test_flat, y_train, y_test)
    train_cnn(x_train_flat, x_test_flat, y_train, y_test)

